# An Overview of Attention Mechanisms in Neural Networks

\[definizione molto generica\]

\[Analogie\]

\[Struttura dell’articolo\]

\[Attenzione additiva\]
Spiega come funzionavano le seq2seq prima
Spiega come l’attenzione ha apportato un miglioramento [schema seq2seq]

\[Attenzione moltiplicativa\]
Miglioramento da parte di Luong et al.
Cambiano le equazioni

\[Auto-attenzione\]
Descrizione dell’architettura Transformer
Struttura dell’auto-attenzione e dei vettori Q V K.
Formula
Spiega che l’Auto-Attenzione è stata proposta con l’invenzione dell’architettura Transformer, anche se in teoria può essere applicata anche ad altre architetture, come modelli Convoluzionali o Ricorrenti

\[Altre menzioni\]
Valuta se inserire questo paragrafo


\[Fonti per approfondire\]
Articoli
Géron, cap sull’attenzione
Lezioni di DL x Stanford
